{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Supervised Learning\n",
    "## Capstone Project: Finding Relationships between Blood Glucose Level and Heart Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of supervised learning is to learn based on the data that is given and confidently predict future results.  Using my Apple Watch and Dexcom Continuous Glucose Monitor (CGM), I am determined to find a relationship between my heart rate (HR) and my blood glucose (BG).  \n",
    "\n",
    "**By finding a relationship, it provides another feature to help predict a users BG outside of the CGM. ** \n",
    "\n",
    "**You will first define the problem you want to solve and investigate potential solutions and performance metrics. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "## I. Definition\n",
    "_(approx. 1-2 pages)_\n",
    "\n",
    "### Project Overview\n",
    "In this section, look to provide a high-level overview of the project in layman’s terms. Questions to ask yourself when writing this section:\n",
    "- _Has an overview of the project been provided, such as the problem domain, project origin, and related datasets or input data?_\n",
    "- _Has enough background information been given so that an uninformed reader would understand the problem domain and following problem statement?_\n",
    "\n",
    "Diabetes is, unfortunately, on the rise in the world.  The number of cases of Type 1 Diabetes (T1D) and Type 2 Diabetes (T2D) has continually gone up since the we have begun to keep track (REF BOOK).  A diabetic is an individual that has lost some (T2D) or all (T1D) of their function islet cells in their pancreas that produce insulin and read BG levels in the blood stream.  Therefore, diabetics must monitor their BG by either drawing blood from their finger and placing that blood on a test strip for a machine to calculate the BG level.  The second way is to use a CGM.  This is a catheter device that is inserted under the skin in the interstitial fluid and can read a diabetics BG every 5 minutes and output it to a device or your phone.  The CGM is a magnificent device that helps diabetes see patterns and trends in their data.  With this data (both forms), a diabetic must determine if their BG is high or low and correct their BG by decreasing the BG with insulin or increasing the BG with carbs.  A lot of diabetics control their insulin intake with an insulin pump.  An insulin pump is catheter device that mechanical supplies insulin to the user through a set of pre-programmed and instant programming settings.  This device is used to simulate the insulin production of the body that diabetics cannot accomplish anymore.  As it can be seen from my description above, life as a diabetic can feel like an eternal game of seesaw.\n",
    "\n",
    "I am a T1D and have been since 1988.  The medical field has done some amazing things to help me have a healthy life, but it is still extremely hard to maintain a healthy BG.  The American Diabetes Association (ADA) (REF) decided to put more funding into hard and software that could help diabetics like myself.  This project is called the Artificial Pancreas (AP) project.  The goal is find a close loop system that can measure the BG with a CGM and output the correct amount of insulin from an insulin pump.    This AP project is currently being worked on by some amazing groups (TypeZero, Medtronic, BigFoot Biomedical, Beta Biomedical).  These companies are writing algorithms to have the CGM and insulin pumps communicate so the BG can be regulated continuous throughout the day.  In a landmark move, Medtronic as produced the first ever FDA approved AP this year (DATE).\n",
    "\n",
    "This work is amazing and very important to me.  However, as an engineer for over 10 years, I find comfort in redundancy.  Therefore, I am looking at my diabetic data in a completely different way to hopefully can use in the next generation of APs.\n",
    "\n",
    "NEED MORE ABOUT MY PROCESS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "In this section, you will want to clearly define the problem that you are trying to solve, including the strategy (outline of tasks) you will use to achieve the desired solution. You should also thoroughly discuss what the intended solution will be for this problem. Questions to ask yourself when writing this section:\n",
    "- _Is the problem statement clearly defined? Will the reader understand what you are expecting to solve?_\n",
    "- _Have you thoroughly discussed how you will attempt to solve the problem?_\n",
    "- _Is an anticipated solution clearly defined? Will the reader understand what results you are looking for?_\n",
    "\n",
    "\n",
    "\n",
    "## Question 1 - Classification vs. Regression\n",
    "*Your goal for this project is to identify students who might need early intervention before they fail to graduate. Which type of supervised learning problem is this, classification or regression? Why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Run the code cell below to load necessary Python libraries and load the student data. Note that the last column from this dataset, 'passed', will be our target label (whether the student graduated or didn't graduate). All other columns are features about each student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Read diabetes data\n",
    "diabetes_data = pd.read_csv(\"DiabetesDataRPB.csv\")\n",
    "print \"Diabetes data read successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Data Exploration\n",
    "Let's begin by investigating the dataset to determine how many students we have information on, and learn about the graduation rate among these students. In the code cell below, you will need to compute the following:\n",
    "+ The total number of data points or readings, n_readings.\n",
    "    + This is readings from each device and manipulated into a cohesive time slot\n",
    "+ The total number of features for each point in time, n_features.\n",
    "+ The number of readings when BG is in range, n_normal.\n",
    "+ The number of readings when BG is out of range (high or low), n_outOfRange.\n",
    "+ The time in range, In_Range in percent (%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of readings: 576\n",
      "Number of features: 4\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of students\n",
    "n_readings = len(diabetes_data[\"Basal Insulin\"])\n",
    "\n",
    "# Calculate number of features\n",
    "n_features = len(diabetes_data.keys())\n",
    "\n",
    "# Print the results\n",
    "print \"Total number of readings: {}\".format(n_readings)\n",
    "print \"Number of features: {}\".format(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature values:\n",
      "   Bolus Taken  Basal Carbs (Liver)  Basal Insulin  Carbs Eaten\n",
      "0          0.0             1.283155       3.849464          0.0\n",
      "1          0.0             1.262734       3.788202          0.0\n",
      "2          0.0             2.576919       3.730758          0.0\n",
      "3          0.0             2.892478       3.677434          0.0\n",
      "4          0.0             3.209471       3.628412          0.0\n"
     ]
    }
   ],
   "source": [
    "feature_cols = list(diabetes_data.columns[:])\n",
    "X_all = diabetes_data[feature_cols]\n",
    "print \"\\nFeature values:\"\n",
    "print X_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features:\n",
    "\n",
    "The next function will add a few more features.\n",
    "\n",
    "These are the constant features that have been loaded.  They will not change for this analysis.  In reality, `'Basal Carbs (liver)'` is very fluctuating and uncertain because the body will produce more carbs if, for example, the body works out.  However, for the data used in this analysis, I went to work and return home without changing my routine.  Therefore, the liver carbs should be pretty close to constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "In this section, you will need to clearly define the metrics or calculations you will use to measure performance of a model or result in your project. These calculations and metrics should be justified based on the characteristics of the problem and problem domain. Questions to ask yourself when writing this section:\n",
    "- _Are the metrics you’ve chosen to measure the performance of your models clearly discussed and defined?_\n",
    "- _Have you provided reasonable justification for the metrics chosen based on the problem and solution?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis\n",
    "_(approx. 2-4 pages)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from basal import Basal \n",
    "from bolusTime import bolusTime\n",
    "from carbCurve import Carbs\n",
    "from carbCurveTime import CarbTIME\n",
    "\n",
    "L = n_readings         # Length of Dataset\n",
    "ISF = -37.7142857143   # Insulin Sensitivity Factor - How much one unit of insulin\n",
    "                       # decreases BG (negative because it decreases BG)\n",
    "C2R = 3.0              # How much 1 gram of Carbs raises BG level\n",
    "\n",
    "# 1 - Bolus\n",
    "basal, bolus = Basal(1,49) # creates curb for bolus ingestion\n",
    "Bolus = bolusTime(bolus, 0, 1, L)\n",
    "\n",
    "# 2 - CARBS\n",
    "carb = Carbs(0.4, 0.0, L) # carb is calculated for an entire length of L\n",
    "CarbsT = CarbTIME(carb, 3, 1)\n",
    "\n",
    "#<---------- \n",
    "# Analysis: Part 3 - Calculate BG \n",
    "#<----------\n",
    "\n",
    "CValue = 76.0  # Initialize a value for BG\n",
    "\n",
    "\n",
    "def create_BG(CValue, diabetes_data, L, bolus, carb):\n",
    "    # Create array for all the boluses that take place\n",
    "    TB = [] # Total boluses combined from ingestion\n",
    "    x = 0\n",
    "    for x in range(L):\n",
    "        TB.append(bolusTime(bolus, x, diabetes_data[\"Bolus Taken\"][x], L))\n",
    "        x += 1 \n",
    "    TB = np.array(TB)\n",
    "    TB = sum(TB)\n",
    "    \n",
    "    # Create array for all the Carbs that take place\n",
    "    TC = []\n",
    "    for x in range(L):\n",
    "        TC.append((CarbTIME(carb, x, diabetes_data[\"Carbs Eaten\"][x])))\n",
    "        x += 1\n",
    "    TC = np.array(TC)\n",
    "    TC = sum(TC)\n",
    "    \n",
    "    # Create array for BG - Target!\n",
    "    DFL = np.array(diabetes_data[\"Basal Carbs (Liver)\"])\n",
    "    DFB = np.array(diabetes_data[\"Basal Insulin\"])\n",
    "    DFB = DFB * -1       # Values decrease blood glucose\n",
    "    DFL = DFL * C2R\n",
    "    TC      = TC * C2R\n",
    "    TB      = TB * ISF\n",
    "    \n",
    "    BG = [0.0]*L\n",
    "    BG.insert(0,CValue)\n",
    "    \n",
    "    for x in range(1, L):\n",
    "        BG[x] = BG[x-1] + DFL[x] + DFB[x] + TC[x] + TB[x]\n",
    "        \n",
    "    del BG[-1]\n",
    "    \n",
    "    return BG\n",
    "\n",
    "BG = create_BG(CValue, diabetes_data, L, bolus, carb)\n",
    "diabetes_data['BG'] = BG\n",
    "\n",
    "#print BG\n",
    "\n",
    "def ML(vector, diabetes_data):\n",
    "    vector = ['carbs, bolus']\n",
    "    \n",
    "    \n",
    "    # Create array for all the boluses that take place\n",
    "    TB = [] # Total boluses combined from ingestion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "In this section, I will prepare the data for modeling, training, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify feature and target columns\n",
    "\n",
    "** *It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.* **\n",
    "\n",
    "Run the code cell below to separate the student data into feature and target columns to see if any features are non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "['Bolus Taken', 'Basal Carbs (Liver)', 'Basal Insulin', 'Carbs Eaten', 'BG']\n",
      "\n",
      "Target column: BG\n",
      "\n",
      "Feature values:\n",
      "   Bolus Taken  Basal Carbs (Liver)  Basal Insulin  Carbs Eaten    BG\n",
      "0          0.0             1.283155       3.849464          0.0  76.0\n",
      "1          0.0             1.262734       3.788202          0.0  76.0\n",
      "2          0.0             2.576919       3.730758          0.0  80.0\n",
      "3          0.0             2.892478       3.677434          0.0  85.0\n",
      "4          0.0             3.209471       3.628412          0.0  91.0\n"
     ]
    }
   ],
   "source": [
    "# Extract feature columns\n",
    "feature_cols = list(diabetes_data.columns[:])\n",
    "\n",
    "# Extract target column 'BG'\n",
    "target_col = diabetes_data.columns[-1] \n",
    "\n",
    "# Show the list of columns\n",
    "print \"Feature columns:\\n{}\".format(feature_cols)\n",
    "print \"\\nTarget column: {}\".format(target_col)\n",
    "\n",
    "# Separate the data into feature data and target data (X_all and y_all, respectively)\n",
    "X_all = diabetes_data[feature_cols]\n",
    "y_all = diabetes_data[target_col]\n",
    "\n",
    "# Show the feature information by printing the first five rows\n",
    "print \"\\nFeature values:\"\n",
    "print X_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Training and Testing Data Split\n",
    "\n",
    "** So far, we have converted all categorical features into numeric values. ** For the next step, we split the data (both features and corresponding labels) into training and test sets. In the following code cell below, you will need to implement the following:\n",
    "+ Randomly shuffle and split the data (X_all, y_all) into training and testing subsets.\n",
    "    + Use 525 training points (roughly 75%) and 173 testing points.\n",
    "    + Set a random_state for the function(s) you use, if provided.\n",
    "    + Store the results in X_train, X_test, y_train, and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 432 samples.\n",
      "Testing set has 144 samples.\n"
     ]
    }
   ],
   "source": [
    "# Import any additional functionality you may need here\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# Set the number of training points\n",
    "num_train = int(n_readings*0.75)\n",
    "\n",
    "# Set the number of testing points\n",
    "num_test = X_all.shape[0] - num_train\n",
    "\n",
    "# TODO: Shuffle and split the dataset into the number of training and testing points above\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all, y_all, test_size=num_test, random_state=1)\n",
    "\n",
    "# Show the results of the split\n",
    "print \"Training set has {} samples.\".format(X_train.shape[0])\n",
    "print \"Testing set has {} samples.\".format(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "In this section, you will choose 3 supervised learning models that are appropriate for this problem and available in scikit-learn. You will first discuss the reasoning behind choosing these three models by considering what you know about the data and each model's strengths and weaknesses. You will then fit the model to varying sizes of training data (100 data points, 200 data points, and 300 data points) and measure the F1 score. You will need to produce three tables (one for each model) that shows the training set size, training time, prediction time, F1 score on the training set, and F1 score on the testing set.\n",
    "\n",
    "**The following supervised learning models are currently available in scikit-learn that you may choose from:**\n",
    "+ Gaussian Naive Bayes (GaussianNB)\n",
    "+ Decision Trees\n",
    "+ Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n",
    "+ K-Nearest Neighbors (KNeighbors)\n",
    "+ Stochastic Gradient Descent (SGDC)\n",
    "+ Support Vector Machines (SVM)\n",
    "+ Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Model Application\n",
    "*List three supervised learning models that are appropriate for this problem. For each model chosen*\n",
    "+ Describe one real-world application in industry where the model can be applied. (You may need to do a small bit of research for this — give references!)\n",
    "+ What are the strengths of the model; when does it perform well?\n",
    "+ What are the weaknesses of the model; when does it perform poorly?\n",
    "+ What makes this model a good candidate for the problem, given what you know about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All my answers below are in assuming classification problems.  These need to be answered as regression\n",
    "\n",
    "# NEEDS WORK\n",
    "\n",
    "1) K-Nearest Neighbors\n",
    "\n",
    "2) Decision Tree Regression with AdaBoost\n",
    "\n",
    "3) SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_regressor(regr, X_train, y_train):\n",
    "    ''' Fits a regressor to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    regr.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print \"Trained model in {:.4f} seconds\".format(end - start)\n",
    "\n",
    "    \n",
    "def predict_BG(regr, X_all, y_all):\n",
    "    ''' Makes predictions using a fit classifier based on R2 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = regr.predict(BG)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print \"Made predictions in {:.4f} seconds.\".format(end - start)\n",
    "    return r2_score(target.values, y_pred)\n",
    "\n",
    "\n",
    "def train_predict(regr, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print \"Training a {} using a training set size of {}. . .\".format(regr.__class__.__name__, len(X_train))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_regressor(regr, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print \"R2 score for training set: {:.4f}.\".format(predict_BG(regr, X_train, y_train))\n",
    "    print \"R2 score for test set: {:.4f}.\".format(predict_BG(regr, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Model Performance Metrics\n",
    "With the predefined functions above, you will now import the three supervised learning models of your choice and run the train_predict function for each one. Remember that you will need to train and predict on each classifier for three different training set sizes: 100, 200, and 300. Hence, you should expect to have 9 different outputs below — 3 for each model using the varying training set sizes. In the following code cell, you will need to implement the following:\n",
    "\n",
    "+ Import the three supervised learning models you've discussed in the previous section.\n",
    "+ Initialize the three models and store them in clf_A, clf_B, and clf_C.\n",
    "    + Use a random_state for each model you use, if provided.\n",
    "    + Note: Use the default settings for each model — you will tune one specific model in a later section.\n",
    "+ Create the different training set sizes to be used to train each model.\n",
    "    + Do not reshuffle and resplit the data! The new training points should be drawn from X_train and y_train.\n",
    "+ Fit each model with each training set size and make predictions on the test set (9 in total).\n",
    "\n",
    "**Note**: Three tables are provided after the following code cell which can be used to store your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a KNeighborsRegressor using a training set size of 200. . .\n",
      "Trained model in 0.0010 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricky\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "query data dimension must match training data dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b8e7ad3ac6ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msize\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mtrain_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"-------------------------------------------------------\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-cea72fc6dbb7>\u001b[0m in \u001b[0;36mtrain_predict\u001b[1;34m(regr, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# Print the results of prediction for both training and testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"R2 score for training set: {:.4f}.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_BG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"R2 score for test set: {:.4f}.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_BG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-cea72fc6dbb7>\u001b[0m in \u001b[0;36mpredict_BG\u001b[1;34m(regr, X_all, y_all)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Start the clock, make predictions, then stop the clock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ricky\\Anaconda2\\lib\\site-packages\\sklearn\\neighbors\\regression.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ricky\\Anaconda2\\lib\\site-packages\\sklearn\\neighbors\\base.pyc\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    397\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[0;32m    398\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[1;32m--> 399\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m             )\n\u001b[0;32m    401\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ricky\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ricky\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ricky\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ricky\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ricky\\Anaconda2\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn/neighbors/binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors.kd_tree.BinaryTree.query (sklearn\\neighbors\\kd_tree.c:10451)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: query data dimension must match training data dimension"
     ]
    }
   ],
   "source": [
    "# TODO: Import the three supervised learning models from sklearn\n",
    "# from sklearn import model_A - Decision Trees used\n",
    "from sklearn import neighbors\n",
    "# from sklearn import model_B\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "# from skearln import model_C\n",
    "from sklearn import svm\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "clf_A = neighbors.KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
    "clf_B = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
    "                          n_estimators=300, random_state=0)\n",
    "clf_C = svm.SVR(kernel='rbf', C=1e3, gamma=0.2)\n",
    "\n",
    "# TODO: Set up the training set sizes\n",
    "X_train_100 = X_train[:200]\n",
    "y_train_100 = y_train[:200]\n",
    "\n",
    "X_train_200 = X_train[:300]\n",
    "y_train_200 = y_train[:300]\n",
    "\n",
    "X_train_300 = X_train[:400]\n",
    "y_train_300 = y_train[:400]\n",
    "\n",
    "# TODO: Execute the 'train_predict' function for each classifier and each training set size\n",
    "# train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "# Per recommendations, I have added the 'for' loop.\n",
    "\n",
    "for size in [200, 300, 400]:\n",
    "    train_predict(clf_A, X_train[:size], y_train[:size], X_test, y_test)\n",
    "    print \"-------------------------------------------------------\"\n",
    "\n",
    "print \"-------------------------------------------------------\"\n",
    "for size in [200, 300, 400]:\n",
    "    train_predict(clf_B, X_train[:size], y_train[:size], X_test, y_test)\n",
    "    print \"-------------------------------------------------------\"\n",
    "\n",
    "print \"-------------------------------------------------------\"\n",
    "for size in [200, 300, 400]:\n",
    "    train_predict(clf_C, X_train[:size], y_train[:size], X_test, y_test)\n",
    "    print \"-------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Results\n",
    "\n",
    "** Classifer 1 - DecisionTreeClassifier**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | R2 Score (train) | R2 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 300               |        0.0050           |        0.0010          |      1.0000      |    0.4947       |\n",
    "| 400               |        0.0020           |        0.0020          |      1.0000      |    0.5472       |\n",
    "| 500               |        0.0020           |        0.0000          |      1.0000      |    0.6284       |\n",
    "\n",
    "** Classifer 2 - AdaBoostClassifier**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | R2 Score (train) | R2 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 300               |        0.7390           |        0.0280          |      0.9712      |    0.5699       |\n",
    "| 400               |        0.9470           |        0.0420          |      0.9621      |    0.6030       |\n",
    "| 500               |        0.9850           |        0.0380          |      0.9501      |    0.6340       |\n",
    " \n",
    "** Classifer 2 - AdaBoostClassifier**  \n",
    "\n",
    "| Training Set Size | Training Time | Prediction Time (test) | R2 Score (train) | R2 Score (test) |\n",
    "| :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |\n",
    "| 300               |        0.7390           |        0.0280          |      0.9712      |    0.5699       |\n",
    "| 400               |        0.9470           |        0.0420          |      0.9621      |    0.6030       |\n",
    "| 500               |        0.9850           |        0.0380          |      0.9501      |    0.6340       |\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Choosing the Best Model\n",
    "In this final section, you will choose from the three supervised learning models the best model to use on the student data. You will then perform a grid search optimization for the model over the entire training set (X_train and y_train) by tuning at least one parameter to improve upon the untuned model's F1 score.\n",
    "\n",
    "## Question 3 - Choosing the Best Model\n",
    "Based on the experiments you performed earlier, in one to two paragraphs, explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 - Model in Layman's Terms\n",
    "In one to two paragraphs, explain to the board of directors in layman's terms how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical or technical jargon, such as describing equations or discussing the algorithm implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Model Tuning\n",
    "Fine tune the chosen model. Use grid search (GridSearchCV) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Create the parameters list you wish to tune\n",
    "n_estimators_range = range(375,450,25) \n",
    "lr_range = [0.2, 0.5, 1.0, 1.5, 2.0]\n",
    "loss_range =  ['linear', 'square', 'exponential']\n",
    "param_grid = dict(n_estimators=n_estimators_range, learning_rate=lr_range, loss=loss_range)\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
    "                          n_estimators=300, random_state=0)\n",
    "\n",
    "# TODO: Make an f1 scoring function using 'make_scorer' \n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# TODO: Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(clf, param_grid=param_grid, scoring=r2_scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print \"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train))\n",
    "print \"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 - Final F1 Score\n",
    "What is the final model's F1 score for training and testing? How does that score compare to the untuned model?\n",
    "\n",
    "Answer: Tuned model has a testing F1 score of 0.8591. This is about 1% pecentage point higher than the default SVM. It is also 4.75% pecentage points higher than the AdaBoost.\n",
    "\n",
    "It is worth noting that I used the StratifiedShuffleSplit cross-validation generator. This was done because we are dealing with an unbalanced dataset. Our target row has 67% of students passing and 33% failing. Therefore the StratisfiedShuffleSplit was used because it creates splits by preserving the same percentage for each target class as in the complete set. - See ref: http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
